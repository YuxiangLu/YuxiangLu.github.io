<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Yuxiang Coding]]></title>
    <url>%2Fundefined%2Fhello-world%2F</url>
    <content type="text"><![CDATA[我会在这里分享我在使用mac、学习自然语言处理的过程中的心得。]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用word2vec训练词向量]]></title>
    <url>%2Fundefined%2Fnlp%2Fword2vec%2F</url>
    <content type="text"><![CDATA[将词转换为向量是目前自然语言处理的基本流程之一。词向量有好几种，本文主要讲述如何利用word2vec将中文词训练为分布式词向量。 语料准备本文使用维基百科文章作为训练语料。使用wget命令下载。 1wget https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2 下载完成后，应该得到一个名为 zhwiki-latest-pages-articles.xml.bz2 的文件。 这个文件是无法直接使用的，需要将其转换为文本，使用python中的gensim模块的WikiCorpus函数处理，完整代码如下： * wiki_to_txt.py 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import loggingimport sysreload(sys)sys.setdefaultencoding('utf-8')import iofrom gensim.corpora import WikiCorpusdef main(): if len(sys.argv) != 2: print("Usage: python3 " + sys.argv[0] + " wiki_data_path") exit() logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) wiki_corpus = WikiCorpus(sys.argv[1], dictionary=&#123;&#125;) texts_num = 0 with io.open("wiki_texts.txt", 'w', encoding='utf-8') as output: for text in wiki_corpus.get_texts(): output.write(b' '.join(text).decode('utf-8') + '\n') texts_num += 1 if texts_num % 10000 == 0: logging.info("已处理 %d 篇文章" % texts_num)if __name__ == "__main__": main() 执行：pyhton wiki_to_txt.py zhwiki-latest-pages-articles.xml.bz2进行处理。 处理结束后得到一个wiki_texts.txt文件。 查看一下这个文件： 发现文章是繁体字（应该是数据集下错了？），没关系，多加一步将繁体转换为中文： 首先，下载两个python文件：zh_wiki.py和langconv.py。 * t2s.py 123456789101112131415# -*- coding: utf-8 -*-from langconv import *def t2s(line): line = Converter('zh-hans').convert(line.decode('utf-8')) line = line.encode('utf-8') return lineif __name__ == "__main__": with open('wiki_texts.txt', 'r') as r: with open('wiki_zhs.txt', 'w') as w: lines = r.readlines() print 'start' for line in lines: w.write(t2s(line.rstrip() + '\n')) python t2s.py运行这个文件后得到简体中文的wiki数据集。处理结果存放在wiki_zhs.txt中。 分词和去除停止词使用 jieba 分词，在jieba_dict文件夹中的dict.txt.big文件可以自己定义一些想要分的词，在stopwords.txt中放入中文的停止词（可选）。完整代码如下： * segment.py 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import jiebaimport loggingimport ioimport sysreload(sys)sys.setdefaultencoding('utf-8')def main(): logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) # jieba custom setting. jieba.set_dictionary('jieba_dict/dict.txt.big') # load stopwords set stopword_set = set() with io.open('jieba_dict/stopwords.txt', 'r', encoding='utf-8') as stopwords: for stopword in stopwords: stopword_set.add(stopword.strip('\n')) output = io.open('wiki_seg.txt', 'w', encoding='utf-8') with io.open('wiki_zhs.txt', 'r', encoding='utf-8') as content: for texts_num, line in enumerate(content): line = line.strip('\n') words = jieba.cut(line, cut_all=False) for word in words: if word not in stopword_set: output.write(word + ' ') output.write(unicode('\n')) if (texts_num + 1) % 10000 == 0: logging.info("已完成前 %d 行的断词" % (texts_num + 1)) output.close()if __name__ == '__main__': main() 处理结果存放在wiki_seg.txt中，如下图： 训练词向量主要使用 gensim 中的 word2vec 训练词向量，完整代码如下： * train.py 123456789101112131415161718# -*- coding: utf-8 -*-import loggingfrom gensim.models import word2vecdef main(): logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) sentences = word2vec.LineSentence("wiki_seg.txt") model = word2vec.Word2Vec(sentences, size=250) # 保存模型，供日后使用 model.save(u"word2vec.model") # 模型读取方式 # model = word2vec.Word2Vec.load("your_model_name")if __name__ == "__main__": main() 训练结果存储在word2vec.model中。 使用这里提供3种word2vec的测试场景：（1）一个词的相似词；（2）两个词的相似词；（3）类比推理。 完整代码如下： * demo.py 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding: utf-8 -*-from gensim.models import word2vecfrom gensim import modelsimport loggingdef main(): logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) model = models.Word2Vec.load('word2vec.model') print("提供 3 种测试模式") print("输入一个词，则去寻找前一百个该词的相似詞") print("输入两个词，则去计算两个词的余弦相似度") print("输入三个词，进行类比推理") while True: query = raw_input("请输入: ") query = query.decode('utf-8') q_list = query.split() try: if len(q_list) == 1: print("相似词前 100 排序") res = model.most_similar(q_list[0], topn=100) for item in res: print(item[0] + "," + str(item[1])) elif len(q_list) == 2: print("计算 Cosine 相似度") res = model.similarity(q_list[0], q_list[1]) print(res) else: print("%s之于%s，如%s之于" % (q_list[0], q_list[2], q_list[1])) res = model.most_similar([q_list[0], q_list[1]], [q_list[2]], topn=100) for item in res: print(item[0] + "," + str(item[1])) print("----------------------------") except Exception as e: print(repr(e))if __name__ == "__main__": main()]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对话系统综述]]></title>
    <url>%2Fundefined%2Fnlp%2Fdialogue_system_review%2F</url>
    <content type="text"><![CDATA[本文主要是对论文：A Survey on Dialogue Systems: Recent Advances and New Frontiers 的理解和翻译。下文中提及的参考文献编号，均可从原论文中找到对应的文献。 综述目前，从应用的角度看，对话系统大致可以分为两类： task-oriented systems（任务型） non-task-oriented systems（闲聊型） 任务型系统主要帮助人们完成确定的任务（例如，订票）。之前广泛应用的方法流程如Figure 1。系统先理解用户的话（nature language understanding，NLU），将其表示成系统可以理解的状态，然后根据策略采取一些action，最后根据这些action转换成自然语言（nature language generating，NLG）回复。这里的NLU使用统计模型来处理。可靠的对话系统仍使用人工特征和规则来表示状态和策略、检测意图等，使得实际使用中的对话系统成本很高，而且很难被用于其他领域。 最近，很多深度学习算法通过学习高维的分布式特征表示来解决这些问题。聊天型对话系统主要使用了这两种模型： 生成模型（generative methods），例如：Seq2Seq model； 检索模型（retrieval-based methods），学习从已有库中选择当前对话对应的回答。 任务型对话系统任务型对话系统主要可以分为两类： 管道型（pipeline） 端到端型（end-to-end） 管道型（Pipeline Methods）管道型对话系统主要包含4个部分： 自然语言理解（NLU） 对话状态追踪器（dialogue state tracking） 策略网络（policy learning） 自然语言生成（NLG） 自然语言理解NLU模块对自然语言进行意图识别和信息抽取，添加到对应的语义槽（semantic slots）。识别意图就是将用户的话语分类到预定义好的类别中。[15，84，112]使用了深度学习进行意图识别。[25，29，74]使用了CNN进行分类。类似的方法同样适用于类别和领域的分类。槽填充（slot filling）是将用户的话中的词打上语义标签（例如，日期、地点等）：[15,17]使用了深度信念网络（DBNs），[51，66,115,113]使用了RNN。NLU的结果（intent和slot）会进一步被对话管理器（dialogue management component）进行处理（对话管理器主要包括对话状态追踪器和策略网络）。 对话状态追踪器在每一轮对话中估计用户的目标。常用的state结构是slot filling或semantic frame。传统方法是使用人工定义的规则来选择最有可能的结果[23]。最近，[26]提出了单领域的，基于深度学习的belief tracking；[58]提出了基于RNN，多领域的tracking模型；[59]提出了neural belief tracker（NBT）来检测slot-value pairs。 策略网络基于状态的表示，policy learning用来生成系统的下一步action。可以使用监督学习和强化学习。[14]使用了深度强化学习，得到了很好的表现。 自然语言生成根据action生成自然语言。[83,94,95,123]使用了基于LSTM的神经网络模型。[20]提出了基于Seq2Seq方法的NLG，[19]扩展了[20]，使得模型能适应用户说话的方式，作出合适的回应。 端到端模型（End-to-End Methods）基于Pipeline方法的系统，有很多在具体领域的人工设计，难以应用到其他领域，并且还有两点局限：一是用户的反馈难以传给模型，二是各个模块间相互依赖（一个模块的输出是另一个模块的输入），适应新环境时修改起来需要很多人力。[7,97]提出了基于神经网络的、端到端的、可训练的任务型对话系统，将对话系统的学习看作是一个从历史对话记录的匹配过程的学习，使用Encoder-Decoder模型来训练整个网络，缺点是监督学习需要大量数据，并且不够健壮。[120]首先提出了端到端的强化学习方法来训DM，优化系统的鲁棒性（系统问用户一系列Yes/No问题来确定答案）。[45]将端到端系统训练为task completion neural dialogue，最终目的是完成一项任务，例如，订电影票。 任务型对话系统通常需要查询外部的知识库。先前的系统一般会发出一次符号查询来获得结果[97,103,45]。[21]使用基于注意力模型（attention）、键值对的检索机制来增强现有循环网络架构。[18]提出了在知识库上的“soft”posteriordistribution来推断用户的兴趣点，取代了符号查询。[102]结合了RNN与领域知识编码的软件和系统action模版。 闲聊型对话系统基于神经网络的生成模型[64]提出了（基于phrase-based Statistical Machine Translation[118]的）概率生成模型，将应答生成看作翻译，但效果不好。随着深度学习在翻译中的应用（Neural Machine Translation）获得成功，激励了在神经网络中生成对话系统的研究。 Sequence-to-Sequence Models主要使用Encoder-Decoder结构，输入X，输出Y。得到中间状态的非线性函数可以是LSTM[27]或GRU（gated recurrent unit）[12]。[5]使用了attention机制提高了性能。[71,12,87,50]使用了RNN的Encoder-Decoder或类似的结构。 对话上下文（context）考虑对话的上下文的能力是建立对话系统的关键。[77]通过连续表示（continuous representation）来表示历史对话记录（包括当前信息），解决了基于上下文的应答生成问题。[52,12]使用了RNN作为decoder，[68]使用了层次（hierarchical）方法,先捕获用户话语的意思，然后将它们合成一句论述。[109]分别使用词和句层面的注意力（attention）机制扩展了hierarchical结构。[82]对比现有方法发现：（1）hierarchical RNN普遍比non-hierarchical性能好，（2）基于上下文信息，神经网络倾向于生成更长更有意义而且多样性的应答。 ============================= 未完待续。。。 应答多样性[77,87,68,38,6,9,114,65,42,86,72,42,38,77,72,57,11,34,33,75,69,68,67,73] 主题及个性化[108,107,13,122,3,86,61,39,119,55] 外部知识库[22,88,116] 交互性对话学习[43,1,104,40,4,41,37] 评价[46,89,56,31,2,46,53,80,47,81,85,32,10,8,44,24,16] 检索模型（Retrieval-base Methods）单轮对话多轮对话融合方法讨论与总结]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>dialogue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在mac上鼓捣终端]]></title>
    <url>%2Fundefined%2Fmac%2Fzsh%2F</url>
    <content type="text"><![CDATA[好看+好用 iTerm2mac必备，官网地址：http://iterm2.org/，或点击直接下载。 安装zsh一般mac会自带zsh，在命令行输入：zsh —version产看版本。 或者直接安装：brew install zsh（Homebrew是mac的安装器）。 使zsh成为默认的shell：命令行输入chsh -s zsh，重启你的iTerm2。 安装Oh My Zsh通过wget安装，首先也要安装wget：brew install wget 在命令行输入： 1sh -c "$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)" 主题oh my zsh自带各种主题，放在~/.oh-my-zsh/themes/文件夹下。 通过vi ~/.zshrc修改其中ZSH_THEME为你喜欢的主题（我比较喜欢默认主题）。 为你的vim安装Solarized或者Tomorrow主题： 1234567891011121314# git下载Solarized源码git clone git://github.com/altercation/solarized.git# 进入刚刚下载好的目录cd solarized/vim-colors-solarized/colors# 创建vim的配置文件夹sudo mkdir -p ~/.vim/colors# 把刚刚下载好的主题复制过去sudo cp solarized.vim ~/.vim/colors/# 创建.vimrc配置文件并修改sudo vim ~/.vimrc# 在.vimrc文件中加入以下几行syntax enableset background=darkcolorscheme solarized 保存.vimrc。 效果如下（不好意思，我用的是Tomorrow主题）： 什么！？怎么没有行号？怎么设置tab键的长短？怎么设置文件表头？ 好吧，给你完整的.vimrc（为vim添加行号、设置tab、设置新建*.sh和*.py文件时的表头）： 123456789101112131415161718192021222324252627282930313233syntax onset background=darkcolorscheme Tomorrow-Night-Eightiesset nu!set tabstop=4set ai!set showmatchset vb t_vb=set autoindentautocmd BufNewFile *.py,*.sh, exec ":call SetTitle()"let $author_name = "Your Name"let $author_email = "Your E-mail"func SetTitle() if &amp;filetype == 'sh' call setline(1,"\#======================================================= ================") call append(line("."), "\# File Name: ".expand("%")) call append(line(".")+1, "\# Author: ".$author_name) call append(line(".")+2, "\# mail: ".$author_email) call append(line(".")+3, "\# Created Time: ".strftime("%c")) call append(line(".")+4, "\#============================================= ================") call append(line(".")+5, "\#!/bin/zsh") call append(line(".")+6, "") else call setline(1, "\# -*- coding: utf-8 -*-") call append(line("."),"\"\"\"") call append(line(".")+1, "\Created Time: ".strftime("%T %x")) call append(line(".")+2, "") call append(line(".")+3, "\Author: ".$author_name) call append(line(".")+4, "\"\"\"") endifendfunc 插件Oh My Zsh支持很多强大的插件，例如git、语法高亮、命令补全等（不过不要加太多的插件，否则会拖慢shell的启动）。 1234567# 下载zsh-syntax-highlighting插件cd ~/.oh-my-zsh/custom/pluginsgit clone git://github.com/zsh-users/zsh-syntax-highlighting.git# 打开.zshrcvi ~/.zshrc# 找到plugins，将你想要的插件添加进去，插件之间以空格隔开plugins=(zsh-syntax-highlighting zsh-autosuggestions git pip python) 保存.zshrc并退出，重启终端或者输入source ~/.zshrc或zsh使配置文件生效。]]></content>
      <categories>
        <category>mac</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>terminal</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
</search>
